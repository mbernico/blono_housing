{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning!\n",
    "\n",
    "Now that we've done our EDA we can build a model.  But wait...what's a model. And what's machine learning?  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference \n",
    "\n",
    "So far we've been doing \"Descriptive Statistics\" which is just statistics that describes the data.  Now we're going to start doing some inferential statistics...  We will infer or predict something given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Model?\n",
    "A model is a just a mathematical function that describes the relationship between variables.  In this case we will create a model that describes the relationship between price and all those other variables...\n",
    "\n",
    "Imagine this model...\n",
    "\n",
    "$$price = bedrooms* beta_1$$\n",
    "\n",
    "This model would describe a very simple relationship between bedrooms and price where we'd just multiple the number of bedrooms by a coefficient $beta_1$ and arrive at price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Ok, but that's not a very good model!\n",
    "\n",
    "It really isn't.  The technical term is 'underspecified.'  Lets build this one instead...\n",
    "\n",
    "$$price = bedrooms* beta_1 + baths_full * beta_2 + baths_half * beta_3 + garage * beta_4 + sqft * beta_6 $$\n",
    "\n",
    "This model would be more fully specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But how do you know what $beta_n$ is?\n",
    "\n",
    "Those beta coefficients are *learnable* parameters.  This is the learning part of machine learning.  Machine Learning is a subset of AI where we construct models and then use data to teach the machine model coefficients (we say parameters).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ok, fine, but how does a machine actually learn?\n",
    "\n",
    "* Step 1 = Initialize $beta_n$ to random values\n",
    "\n",
    "* Step 2 = measure how wrong the model is for each observation\n",
    "    * This is called a loss function\n",
    "    * $(\\hat{y} - y)^2$   the squared difference between the predicted price and actual\n",
    "    * So the average loss is called the 'cost function' and it looks like this:\n",
    "        * $$J = \\frac{1}{2m} \\sum_{i=1}^{m}( \\hat{y} - y)^2 $$\n",
    "        \n",
    "* Step 3 = Now that we know how wrong we are, we can nudge $beta_n$ in the correct direction\n",
    "    * The correct direction is the slope of the cost function w.r.t J\n",
    "    * We call this the gradient...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How does a machine actually learn (part 2)\n",
    "\n",
    "So then we do this thing called gradient descent.\n",
    "\n",
    "Repeat Until Converged: {\n",
    "$$\\beta = \\beta-\\alpha\\frac{\\partial}{\\partial\\beta}J(\\beta)$$\n",
    "}\n",
    "\n",
    "So, every time we update theta, we will set theta equal to the previous value minus $\\alpha$ (a learning rate) multiplied by the partial derivative of the cost function J, with respect to $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## If you don't understand this math that's ok, here's what you should remember...\n",
    "1.  There is a point to calculus.\n",
    "2.  We start with random betas and then we update them over and over again until we make the cost function as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So lets build a model already!\n",
    "\n",
    "Overall plan:\n",
    "* Load our data\n",
    "* Test/Train Split\n",
    "* Train our model\n",
    "* Measure it's goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load our data\n",
    "df = pd.read_csv(\"../data/sfh_house_data.csv\", index_col=0)\n",
    "df = df[[\"baths_full\", \"baths_half\", \"bedrooms\",\"garage\", \"sqft\", \"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train = (803, 5)\n",
      "y_train = (803,)\n",
      "X_test = (201, 5)\n",
      "y_test = (201,)\n"
     ]
    }
   ],
   "source": [
    "# Train / Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "y = df['price']\n",
    "X = df.drop('price', axis=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "print(\"X_train =\",X_train.shape)\n",
    "print(\"y_train =\",y_train.shape)\n",
    "print(\"X_test =\",X_test.shape)\n",
    "print(\"y_test =\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = baths_full*21486.3693338 + baths_half*6164.1007002 + bedrooms*6546.61839286 + garage*17800.6055221 + sqft*76135.7463411 + 221388.590509\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "## this part just prints the model, it's complicated you can ignore it.\n",
    "model_coefs = zip(X.columns, model.coef_)\n",
    "model_string = \"price = \"\n",
    "for coef in model_coefs:\n",
    "    model_string = model_string + str(coef[0]) + \"*\" + str(coef[1]) + \" + \"\n",
    "model_string = model_string + str(model.intercept_)\n",
    "print(model_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$41,087.00\n"
     ]
    }
   ],
   "source": [
    "# Lets measure how good the model is, using the test set\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_hat = model.predict(X_test)\n",
    "print('${:,.2f}'.format(mean_absolute_error(y_test, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
